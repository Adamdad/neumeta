# Experiment settings
base_config: toy/experiments/base_config.yaml
experiment:
  name: ninr_transformer_cifar10_32-64-4layer-5
  num_epochs: 30
  log_interval: 25
  seed: 42  # Random seed

# Model configurations
model:
  type: ResNet20
  pretrained_path: resnet20-12fca82f.th  # Path to pre-trained model

# Training configurations
training:
  learning_rate: 1e-3
  batch_size: 64
  coordinate_noise: 1.0
  weight_decay: 1e-2
  clip_grad: 10.0
  save_model_path: toy/experiments/transformer_cifar10_32-64-4layer-5 # Directory to save the trained model

# Hyper-model specifications (specific to NeRF)
hyper_model:
  type: transformer  # Specify that the hyper-model is of type transformer
  input_dim: 6
  hidden_dim: 128
  num_layers: 4
  nhead: 4  # Number of heads for multi-head attention
  dim_feedforward: 128  # Dimension of the feedforward network in the transformer
  output_dim: 9
  normalizing_factor: 1.0  # Factor for normalizing positional embeddings

# Experiment dimensions configuration
dimensions:
  range: [32, 64]  # Dimension range for the experiment
  test: 32  # Testing dimension
  norm: 64  # Normalization factor
  start: 64  # Starting dimension
